---
layout: single
title: "가상머신2"
categories: python
tag: [linux, virtualmachine]
toc: true
author_profile: false
sidebar:
  nav: "docs"
typora-root-url: ../
---

# 가상머신 프로그램

### 가상머신 실행 + putty 실행

**찌꺼기 폴더 삭제**

- 전부 다]
  - rm -rf ~/hadoopTmpData

**하둡 폴더로 이동** 

- NN만]
- cd ~/hadoop/hadoop-3.2.2

**하둡시스템 포맷**

NN만]

- bin/hadoop namenode -format
- bin/hadoop datanode -format

**NN만]**

- sbin/start-all.sh

**확인** 

- 모두 다]
- jps >> 확인
- NN] 
  - Jps
  - NodeManager
  - ResourceManager
  - NameNode
  - DataNode
- DN]
  - DataNode
  - NodeManager
  - Jps
  - SecondaryNameNode

**끄기**

  - NN]

  - sbin/stop-all.sh

<hr>

> gutenberg.org

- 원하는 소설 찾아서
- 텍스트 UTF-8로 저장하기

- 그 텍스트 파일을 NN의 알드라이브로 옮기기 

  - NN] ~/hadoop/hadoop-3.2.2로 

  - NN의 HDD 확인 : ls

- HDFS확인 
  - bin/hadoop fs -ls /

- HDFS로 파일 업로드
  - bin/hadoop fs -put 파일명 HDFS경로
  - bin/hadoop fs -put Troi.txt /

<hr>

**maven repository >> hadoop-common(Apache Hadoop Common/3.3.3/.jar), hadoop-mapreduce-client-core(Apache Hadoop MapReduce Core/3.3.3/.jar) 다운**

- src - Build Path - Configure Build Path
- Library tab - Add External JARs
- 다운받은 2개 추가

> Package - com.oz.jun121.wc
>
> Name - WCMapper

```java
package com.oz.jun121.wc;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

// Hadoop 작업의 첫번째 단계 : Map

// 1단계 : 다운받은 소설책을 분석하라고 넣어줄 것! 
//		>> Key : 없고, Value : String (Hadoop에서는 String이 Text)
// 2단계 : 결과를 받아오기 
//		>> Key : String, Value : int (Hadoop에서는 int가 IntWritable)
//		ex) I, 1
//			am, 1
//			sleepy, 1

public class WCMapper extends Mapper<Object, Text, Text, IntWritable> {
	
	// 결과처리를 위해 자료형을 맞춰주려면 아예 메소드 밖에 빼서 자료형을 맞춰야...
	// 메모리 절약하기 위해서 => singleton 처리 
	private static final Text WORD = new Text();
	private static final IntWritable ONE = new IntWritable(1);
	
	
	// map을 override >> 한 문장마다 이 method가 호출이 될 것 
	@Override
	protected void map(Object key, // data의 유무 체크(별로 중요하지는 않음)
			Text value, // ** 중요 ** 그 문장 자체 << 해당하는 문장을 가져오기 위함
			Mapper<Object, Text, Text, IntWritable>.Context context) // 결과처리용
			throws IOException, InterruptedException {
		
		// 기존에 사용하던 String 객체로 바꿔주는 작업 
		String line = value.toString();
		
		// 정확하게 단어의 위치를 체크해야 할 때 : split
		
		// 지금 우리가 하려는 것 : 단순히 단어체크 : StringTokenizer
		StringTokenizer st = new StringTokenizer(line, " ");
		
		while (st.hasMoreTokens()) { // 반복문 돌려서
			// 결과처리... (자료형을 맞춰줘야)
			WORD.set(st.nextToken());
			context.write(WORD, ONE);
		}
	}
}
```

> Package - com.oz.jun121.wc
>
> Name - WCReducer

```java
package com.oz.jun121.wc;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

// Hadoop 작업의 두번째 단계 : Combine 
// 	는 알아서 처리 됨 

// Hadoop 작업의 세번째 단계 : Reduce
//		in : sleepy <1, 1, 1> 	>> 앞에 두자리
//		out : sleepy 3			>> 뒤에 두자리
									
										// 앞의 두자리는 Mapper쪽 뒤에 두자리와 같아야함!!
public class WCReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

	// 메모리 아껴주기 위한 singleton 처리 (결과처리)
	private static final IntWritable SUM = new IntWritable();
	
		@Override
		protected void reduce(Text arg0, // map 단계에서 해준 key값 : sleepy
				Iterable<IntWritable> arg1, // list 비슷한 : <1, 1, 1>
				Reducer<Text, IntWritable, Text, IntWritable>.Context arg2) // 결과처리용  
						throws IOException, InterruptedException {

			int sum = 0;
		// <1, 1, 1> 합쳐주는 작업 
			for (IntWritable i : arg1) {
				// sum은 int, i는 IntWritable => 형 변환을 해줘야...!
				// => .get();
				sum += i.get(); // 1 + 1 + 1 = 3
			}
			SUM.set(sum);
			arg2.write(arg0, SUM);			
		}
}
```

> Eclipse - Java project 생성 (Jun12_1_WordCount)
>
> Package - com.oz.jun121.wc
>
> Name - WCMain 

```java
package com.oz.jun121.wc;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

//소설책 => 작가가 무슨 단어를 자주 사용했는지 

//Hadoop : 서버급(Linux) 컴퓨터를 여러대로 병렬처리해서 분석을 하는 Java프로그램

//Windows : 자동완성 하려고 ...!
//		=> 필요한 것만 자동완성되면 OK
//		=> Hadoop 전체적인 것 필요 X
//		=> 실제로 실행할 Linux에는 Hadoop전체가 다 설치되어 있음
//		=> hadoop-common, hadoop-mapreduce-client-core

// Hadoop의 마지막 단계 
public class WCMain {
	public static void main(String[] args) {
		try {
			// 설정
			Configuration c = new Configuration();
			
			// 하둡작업을 하겠다. 라고 알리는
			Job j = Job.getInstance(c);
			
			// 각 단계별 담당 클래스 지정
			j.setMapperClass(WCMapper.class); 		// 이 작업의 Mapper 역할은 누가?
			j.setCombinerClass(WCReducer.class);	// 이 작업의 Combine 역할은 누가?
			j.setReducerClass(WCReducer.class); 	// 이 작업의 Reduce 역할은 누가?
			
			// 결과 형태 - Reduce쪽 뒤에 두개 (keyOut, valueOut)
			j.setOutputKeyClass(Text.class);
			j.setOutputValueClass(IntWritable.class);
			
			// HDFS 최상위에 있는 txt파일을 분석
			FileInputFormat.addInputPath(j, new Path("/Troi.txt"));
			
			// 분석해서 그 결과를 HDFS 최상위에 있는 지정한 폴더에 담도록 
			FileOutputFormat.setOutputPath(j, new Path("/sResult"));
			
			// *** 작업 끝날때까지 대기(필수적으로 명시!!!)
			j.waitForCompletion(true);
	
		} catch (Exception e) {
			e.printStackTrace();
		}
	}
}
```

- WCMain클래스 실행(오류O)
- Export - Runnable JAR file - wc.jar
- 알드라이브 - Troi.txt 있는 곳에 wc.jar복붙  



- 실행

  - wc.jar
    - ~/hadoop/hadoop-3.2.2$ 경로
    - bin/hadoop jar 파일명.jar
    - bin/hadoop jar wc.jar

  - tk.jar
    - bin/hadoop jar 파일명.jar 분석할파일명 결과폴더명
    - bin/hadoop jar tk.jar /Troi.txt/sResult2

- HDFS 확인
  - bin/hadoop fs -ls /
  - /sResult 폴더 생겼는지 확인 

실행하면 결과폴더/part-r-00000 결과파일을 만들어 줌 

- 결과 파일 가져오기 
  - bin/hadoop fs -get HDFS에 있는 뭔가를 NN의 어디로
  - bin/hadoop fs -get /sResult/part-r-00000 sResult.txt
  - ls로 Linux로 가져와졌는지

그 결과파일을 알드라이브로 WinPC쪽에 가져와서 확인 

_결과가 안나오거나 잘못됐다면_

- jps 잘나오는지 확인 
- jps잘 안나온다면 세팅부분 다시 검토
- jps잘 나오는데 결과가 안나온다면 자바코드 검토

실행할 때 결과폴더가 없어야함

- bin/hadoop fs -rm -r HDFS경로
- bin/hadoop fs -rm -r /sResult

<hr>

**R스튜디오**

```R
dResult <- readLines("C:\\Users\\오승우\\Desktop\\Sol_Python_File\\sResult.txt", encoding="UTF-8")
dResult <- strsplit(dResult, "\t")
dResult
dResult[[100]]
dResult[[100]][1]
dResult[[100]][2]

word = c()
count = c()

for (d in dResult) {
  # print(d[1])
  word[length(word) + 1] = d[1]
  count[length(count) + 1] = as.numeric(d[2])
}

word
count

dDF <- data.frame(word=word, count=count)
dDF

install.packages("devtools")
library(devtools)
devtools::install_github("lchiffon/wordcloud2")
library(wordcloud2)

dDF <- dDF[order(-dDF$count), ]
dDF
wordcloud2(dDF)
wordcloud2(dDF, size=5, fontFamily="D2Coding")
wordcloud2(dDF, fontFamily = "D2Coding", color="random-light", shape="star")
```

<hr>

> 삼국지 1권~10권
> 유비(현덕), 조조(맹덕), 손권(중모) 언급 횟수 카운트하는 프로그램을 만들어서...


Hadoop 실행으로 결과 뽑기!

- 분석용 파일을 HDFS 최상위로 업로드
  - bin/hadoop fs -put Three* /(Three* : Three로 시작하는거 전부)

- 확인
  - bin/hadoop fs -ls / 

- 실행
  - bin/hadoop jar jar파일명 /저장할폴더명
  - bin/hadoop jar tk.jar /tkResult

- 확인 
  - bin/hadoop fs -ls /

결과파일 Linux로 가져와서

- bin/hadoop fs -get /저장폴더/part-r-00000 파일명.확장자
  - bin/hadoop fs -get /tkResult/part-r-00000 tkResult.txt



```R
# Hadoop 결과 파일 불러오기
tkResult <- readLines("C:\\Users\\오승우\\Desktop\\Sol_Python_File\\tkResult.txt", encoding="UTF-8")
tkResult

tkResult <- strsplit(tkResult, "\t")
tkResult

name = c()
count = c()

for (tr in tkResult) {
  name[length(name) + 1] = tr[1]
  count[length(count) + 1] = as.numeric(tr[2])
}

name
count

# dataframe으로 !!
trDF = data.frame(name=name, count=count)
trDF

# barplot
barplot(
  trDF$count,
  names.arg = trDF$name,
  main = "삼국지",
  xlab = "인물",
  ylab = "언급 횟수",
  col = c('#D1B2FF', '#B5B2FF', '#B2CCFF')
)

# pie
trDF <- trDF[order(-trDF$count), ]
trDF
pie(
  trDF$count,
  labels = trDF$name,
  main = "삼국지",
  col = c('#D1B2FF', '#B5B2FF', '#B2CCFF')
)
```

