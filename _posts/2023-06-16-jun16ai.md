---
layout: single
title: "AI"
categories: python
tag: [colab, keras, tokenization]
toc: true
author_profile: false
sidebar:
  nav: "docs"
typora-root-url: ../
---

# 인공지능(Artificial Intelligence)

컴퓨터를 학습시켜서 자동으로 개선시킬 수 있도록 컴퓨터 알고리즘 연구 : 기계학습 

AI > ML > DL 

- AI (Artificial Intelligence)
  - 인간의 지능과 유사한 기능을 가진 컴퓨터 시스템 
  - 인간의 지능을 인공적으로 컴퓨터에 심어놓은것 

- ML (Machine Learning)
  - 학습 모형을 기반으로 해서 외부에서 들어오는 data들을 스스로 학습하는 것 
  - 빅데이터를 분석하거나 가공해서 새로운 정보를 깨우치거나, 미래 예측이 가능하도록 하는 기술 
  - 쌓아둔 데이터를 토대로 서로간의 상관관계, 특성 등을 찾아냄 

- DL (Deep Learning)
  - 컴퓨터가 다양한 데이터를 기반으로 해서 사람처럼 [스스로] 학습할 수 있도록 하는 [인공신경망]을 구축하는 것 
  - 축적된 데이터에 국한되는 것이 아니라, 데이터를 스스로 학습까지 하는 기계학습능력을 키워서 결과를 도출 

### Text Preprocessing - 텍스트 전처리 

내가 해결하고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리해버리는 방법 

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
nltk.download('punkt')
```

### 토큰화(Tokenization)

어떤 문장을 단어로 잘라내서 정제하고, 정규화하고... 

\- 구두점(Functuation)

 \- 마침표, 쉼표, 물음표, 느낌표, 세미콜론, ...

```python
text = "A barking dog never bites. A big fish in a small pond. No pain No gain"
```

```python
print(word_tokenize(text))
# word_tokenize : you're => you와 're로 구분 / Don't => Do와 n't로 구분
#             => 구두점을 별도로 표시 
print()
print(WordPunctTokenizer().tokenize(text))
#             => 구두점을 별도로 표시 
print()
print(text_to_word_sequence(text))
# keras의 text_to_word_sequence : 모든 알파벳을 소문자로 바꿔줌 
#                                 구두점 제거
#                                 you're, don't, ... 같은 경우에는 보존함 
```

```
['you', "'re", 'A', 'barking', 'dog', 'never', 'bites', '.', 'A', 'big', 'fish', 'in', 'a', 'small', 'pond', '.', 'No', 'pain', 'No', 'gain']

['you', "'", 're', 'A', 'barking', 'dog', 'never', 'bites', '.', 'A', 'big', 'fish', 'in', 'a', 'small', 'pond', '.', 'No', 'pain', 'No', 'gain']

["you're", 'a', 'barking', 'dog', 'never', 'bites', 'a', 'big', 'fish', 'in', 'a', 'small', 'pond', 'no', 'pain', 'no', 'gain']
```

### 문장 토큰화(Sentence Tokenization)

```python
sentence = """This eBook is for the use of anyone anywhere in the United States and
most other parts of the world at no cost and with almost no restrictions
whatsoever. You may copy it, give it away or re-use it under the terms
of the Project Gutenberg License included with this eBook or online at
www.gutenberg.org. If you are not located in the United States, you
will have to check the laws of the country where you are located before
using this eBook.
"""
sentence
```

```
This eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org. If you are not located in the United States, you\nwill have to check the laws of the country where you are located before\nusing this eBook.\n
```

```python
from nltk.tokenize import sent_tokenize 
```

```python
sent_tokenize(sentence)
# NLTK는 단순하게 마침표로 문장을 구분하지 않음 
# Dr. , St. , Mrs. , ... 등 단어들은 마침표를 기준으로 해서 나눠지지 않음 ! 
```

```
['This eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever.',
 'You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org.',
 'If you are not located in the United States, you\nwill have to check the laws of the country where you are located before\nusing this eBook.']
```

```python
# KSS(Korean Sentence Spliter)
!pip install kss 
```

```python
import kss 
```

```python
kor = "오늘부터 AI 시작. 텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높다. 한번 경험해보자."
kor
```

```
오늘부터 AI 시작. 텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높다. 한번 경험해보자.
```

```python
print(kss.split_sentences(kor))
```

```
WARNING:root:Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D
For your information, Kss also supports mecab backend.
We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.
Please refer to following web sites for details:
- mecab: https://github.com/hyunwoongko/python-mecab-kor
- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class

['오늘부터 AI 시작이에요.', '텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요.', '한번 경험해봅시다.']
```

### 한국어 = 교착어 (어근 + 접사)

한국어에는 [조사]가 존재 

- 글자 뒤에 띄어쓰기가 없이 존재
- 형태소 (morpheme)
  - 말의 가장 작은 단위 
  - 자립형태소 : 명사, 대명사, 부사, 수사, 관형사, ...
  - 의존형태소 : 다른 형태소와 결합을 해야만 하는 ... 어간, 어미, 접사, 조사, ... 

### 품사 태깅(Part-of-speech tagging) : 단어 토큰화를 거친 토큰들(단어들)에게 품사를 붙여주는 작업 

동음이의어 

mean : 동사] 의미하다 / 형] 비열한,못된 / 명사] 평균 

연패 : 연속해서 패하다 / 연속해서 이기다 

# NLTK / KoNLPy

```python
nltk.download('averaged_perceptron_tagger') # 품사 태깅을 위한 라이브러리 
```

```python
from nltk.tag import pos_tag
```

```python
text = "A barking dog never bites. A big fish in a small pond. No pain No gain"
tokenized_sentence = word_tokenize(text)
print(tokenized_sentence)
print(pos_tag(tokenized_sentence))
```

```
['A', 'barking', 'dog', 'never', 'bites', '.', 'A', 'big', 'fish', 'in', 'a', 'small', 'pond', '.', 'No', 'pain', 'No', 'gain']
[('A', 'DT'), ('barking', 'NN'), ('dog', 'NN'), ('never', 'RB'), ('bites', 'VBZ'), ('.', '.'), ('A', 'DT'), ('big', 'JJ'), ('fish', 'NN'), ('in', 'IN'), ('a', 'DT'), ('small', 'JJ'), ('pond', 'NN'), ('.', '.'), ('No', 'DT'), ('pain', 'NN'), ('No', 'RB'), ('gain', 'NN')]

```

### NNP : 고유명사 

### PRP : 인칭대명사

### RP : 부사

### VBP : 단수, 현재형, 3인칭 동사 

### W ~ : wh ~ 

### JJ : 형용사 

### NN : 단수명사 

### NNS : 복수명사 

### MD : 조동사 

### VB : 동사 기본형 

### VBD : 동사 과거시제 

### VBG : 동명사 



# 한국어 자연어처리 : KoNLPy라는 파이썬 패키지

KoNLPy에서 사용할 수 있는 형태소 분석기 
- Okt (Open Korea Text)
- Komoran
- kkma(꼬꼬마)
- Mecab
- Hannanum

```python
!pip install konlpy
```

```python
from konlpy.tag import Okt
from konlpy.tag import Kkma
```

```python
okt = Okt()

print(okt.morphs("오늘은 금요일이고, 내일은 주말이다!"))
# morphs : 형태소 분석 : 어떤 대상 어절을 최소 의미단위인 형태소로 분석하는 것 
print(okt.pos("오늘은 금요일이고, 내일은 주말이다!"))
# pos : 품사 태깅(Part-of-Speech tagging)
print(okt.nouns("오늘은 금요일이고, 내일은 주말이다!"))
# nouns : 명사 추출
```

```
['오늘', '은', '금요일', '이고', ',', '내일', '은', '주말', '이다', '!']
[('오늘', 'Noun'), ('은', 'Josa'), ('금요일', 'Noun'), ('이고', 'Josa'), (',', 'Punctuation'), ('내일', 'Noun'), ('은', 'Josa'), ('주말', 'Noun'), ('이다', 'Josa'), ('!', 'Punctuation')]
['오늘', '금요일', '내일', '주말']
```

```python
kkma = Kkma()

print(kkma.morphs("오늘은 금요일이고, 내일은 주말이다!"))
print(kkma.pos("오늘은 금요일이고, 내일은 주말이다!"))
print(kkma.nouns("오늘은 금요일이고, 내일은 주말이다!"))
```

```
['오늘', '은', '금요일', '이', '고', ',', '내일', '은', '주말', '이', '다', '!']
[('오늘', 'NNG'), ('은', 'JX'), ('금요일', 'NNG'), ('이', 'VCP'), ('고', 'ECE'), (',', 'SP'), ('내일', 'NNG'), ('은', 'JX'), ('주말', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('!', 'SF')]
['오늘', '금요일', '내일', '주말']
```

# 코퍼스(Corpus) : 말뭉치

코퍼스에서 용도에 맞게 토큰을 나누는 것을 토큰화(Tokenization)

토큰화를 진행하기 전, 후에 텍스트를 용도에 맞게 정제(Cleaning), 정규화(Normalization)를 하는 것이 필요 

- 정제(Cleaning) : 가지고 있는 말뭉치에서 노이즈 데이터를 제거 
- 정규화(Normalization) : 표현 방법이 서로 다른 단어들을 통일시켜서 같은 단어로 재가공 
  1. 규칙에 따라서 표기가 다른 언어를 통합시키기 
  ex) US USA us U.S.A 
  2. 대소문자를 통합 





